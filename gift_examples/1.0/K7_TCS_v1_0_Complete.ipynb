{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K\u2087 Metric Reconstruction v1.0 - Complete TCS Implementation\n",
    "\n",
    "**Torsion Cohomology Solver (TCS) for G\u2082 Manifolds**\n",
    "\n",
    "100% self-contained notebook - no external files needed.\n",
    "\n",
    "## Features\n",
    "\n",
    "- **Five-phase curriculum learning** (15,000 epochs)\n",
    "- **Complete harmonic basis extraction**: b\u2082=21, b\u2083=77\n",
    "- **Yukawa tensor computation**: Y_\u03b1\u03b2\u03b3 [21\u00d721\u00d777]\n",
    "- **Geometric validation**: Ricci-flatness, holonomy tests\n",
    "- **Calibration constraints**: Associative and coassociative cycles\n",
    "- **Adaptive loss scheduling**: Automatic weight adjustment\n",
    "\n",
    "## Quick Start\n",
    "\n",
    "1. **Runtime** \u2192 Change runtime type \u2192 **GPU** (T4/A100)\n",
    "2. **Runtime** \u2192 Run all\n",
    "3. **Download results** before session ends\n",
    "\n",
    "## Target Metrics\n",
    "\n",
    "- Torsion closure: < 1\u00d710\u207b\u00b3\n",
    "- Torsion coclosure: < 1\u00d710\u207b\u00b3\n",
    "- Yukawa deviation: < 10%\n",
    "- Harmonic bases: Full rank (21 and 77)\n",
    "\n",
    "**Framework:** GIFT v2.0  \n",
    "**Version:** 1.0  \n",
    "**Updated:** 2025-01-18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# SETUP AND INSTALLATION\n",
    "# ============================================================\n",
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "print('Installing required packages...')\n",
    "!pip install -q torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "!pip install -q tensorly matplotlib seaborn numpy scipy tqdm\n",
    "print('Installation complete\\n')\n",
    "\n",
    "# Setup directories (Colab local storage)\n",
    "WORK_DIR = Path('/content/K7_v1_0_training')\n",
    "WORK_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "CHECKPOINT_DIR = WORK_DIR / 'checkpoints'\n",
    "CHECKPOINT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "RESULTS_DIR = WORK_DIR / 'results'\n",
    "RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f'Working directory: {WORK_DIR}')\n",
    "print('NOTE: All data stored in /content/ - download before session ends!')\n",
    "print('='*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# IMPORTS AND DEVICE CONFIGURATION\n",
    "# ============================================================\n",
    "\n",
    "import json\n",
    "import time\n",
    "import warnings\n",
    "from typing import Dict, List, Tuple, Optional, Any\n",
    "from itertools import permutations\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR, LinearLR, SequentialLR\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Device configuration\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'\\nDevice: {DEVICE}')\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f'GPU: {torch.cuda.get_device_name(0)}')\n",
    "    print(f'Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB')\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "else:\n",
    "    print('WARNING: No GPU detected - training will be very slow!')\n",
    "    print('Go to Runtime > Change runtime type > GPU')\n",
    "\n",
    "print('='*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CONFIGURATION\n",
    "# ============================================================\n",
    "\n",
    "CONFIG = {\n",
    "    'version': 'v1.0_complete_tcs',\n",
    "    'seed': 42,\n",
    "    \n",
    "    # GIFT theoretical parameters\n",
    "    'gift_parameters': {\n",
    "        'tau': 3.8967452300785634,\n",
    "        'xi': 0.9817477042468103,\n",
    "        'epsilon0': 0.125,\n",
    "        'b2': 21,\n",
    "        'b3': 77,\n",
    "    },\n",
    "    \n",
    "    # Neural network architecture\n",
    "    'architecture': {\n",
    "        'phi_network': {\n",
    "            'hidden_dims': [384, 384, 256],\n",
    "            'n_fourier': 32\n",
    "        },\n",
    "        'harmonic_h2_network': {\n",
    "            'hidden_dim': 128,\n",
    "            'n_fourier': 24,\n",
    "            'n_forms': 21\n",
    "        },\n",
    "        'harmonic_h3_network': {\n",
    "            'hidden_dim': 128,\n",
    "            'n_fourier': 24,\n",
    "            'n_forms': 77\n",
    "        }\n",
    "    },\n",
    "    \n",
    "    # Training configuration\n",
    "    'training': {\n",
    "        'total_epochs': 15000,\n",
    "        'batch_size': 2048,\n",
    "        'grad_accumulation': 4,\n",
    "        'lr': 1e-4,\n",
    "        'weight_decay': 1e-4,\n",
    "        'grad_clip': 1.0,\n",
    "        'warmup_epochs': 500,\n",
    "        \n",
    "        # Five-phase curriculum\n",
    "        'curriculum': {\n",
    "            'phase1_neck_stability': {\n",
    "                'range': [0, 2000],\n",
    "                'grid_n': 8,\n",
    "                'loss_weights': {\n",
    "                    'torsion_closure': 0.5,\n",
    "                    'torsion_coclosure': 0.5,\n",
    "                    'volume': 2.0,\n",
    "                    'gram_h2': 1.0,\n",
    "                    'gram_h3': 0.5,\n",
    "                    'boundary': 0.5,\n",
    "                    'calibration': 0.0\n",
    "                }\n",
    "            },\n",
    "            'phase2_acyl_matching': {\n",
    "                'range': [2000, 5000],\n",
    "                'grid_n': 8,\n",
    "                'loss_weights': {\n",
    "                    'torsion_closure': 1.0,\n",
    "                    'torsion_coclosure': 1.0,\n",
    "                    'volume': 0.5,\n",
    "                    'gram_h2': 1.5,\n",
    "                    'gram_h3': 1.0,\n",
    "                    'boundary': 1.5,\n",
    "                    'calibration': 0.0\n",
    "                }\n",
    "            },\n",
    "            'phase3_cohomology_refinement': {\n",
    "                'range': [5000, 8000],\n",
    "                'grid_n': 10,\n",
    "                'loss_weights': {\n",
    "                    'torsion_closure': 2.0,\n",
    "                    'torsion_coclosure': 2.0,\n",
    "                    'volume': 0.2,\n",
    "                    'gram_h2': 3.0,\n",
    "                    'gram_h3': 2.0,\n",
    "                    'boundary': 2.0,\n",
    "                    'calibration': 0.5\n",
    "                }\n",
    "            },\n",
    "            'phase4_harmonic_extraction': {\n",
    "                'range': [8000, 10000],\n",
    "                'grid_n': 10,\n",
    "                'loss_weights': {\n",
    "                    'torsion_closure': 3.0,\n",
    "                    'torsion_coclosure': 3.0,\n",
    "                    'volume': 0.1,\n",
    "                    'gram_h2': 5.0,\n",
    "                    'gram_h3': 3.0,\n",
    "                    'boundary': 1.5,\n",
    "                    'calibration': 1.0\n",
    "                }\n",
    "            },\n",
    "            'phase5_calibration_finetune': {\n",
    "                'range': [10000, 15000],\n",
    "                'grid_n': 12,\n",
    "                'loss_weights': {\n",
    "                    'torsion_closure': 5.0,\n",
    "                    'torsion_coclosure': 5.0,\n",
    "                    'volume': 0.05,\n",
    "                    'gram_h2': 5.0,\n",
    "                    'gram_h3': 4.0,\n",
    "                    'boundary': 1.0,\n",
    "                    'calibration': 3.0\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    \n",
    "    # Checkpointing\n",
    "    'checkpointing': {\n",
    "        'interval': 500,\n",
    "        'keep_best': 5,\n",
    "        'auto_resume': True\n",
    "    },\n",
    "    \n",
    "    # Validation\n",
    "    'validation': {\n",
    "        'interval': 100,\n",
    "        'ricci_interval': 500,\n",
    "        'ricci_points': 1000\n",
    "    },\n",
    "    \n",
    "    # Yukawa computation\n",
    "    'yukawa_computation': {\n",
    "        'n_mc_samples': 20000,\n",
    "        'grid_n': 10,\n",
    "        'tucker_rank': [3, 3, 3],\n",
    "        'antisymmetry_tolerance': 1e-6\n",
    "    },\n",
    "    \n",
    "    # Holonomy test\n",
    "    'holonomy_test': {\n",
    "        'n_loops': 10,\n",
    "        'n_steps_per_loop': 50,\n",
    "        'preservation_tolerance': 1e-4\n",
    "    }\n",
    "}\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(CONFIG['seed'])\n",
    "torch.manual_seed(CONFIG['seed'])\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(CONFIG['seed'])\n",
    "\n",
    "# Save configuration\n",
    "with open(WORK_DIR / 'config.json', 'w') as f:\n",
    "    json.dump(CONFIG, f, indent=2)\n",
    "\n",
    "print('\\nConfiguration initialized')\n",
    "print(f'Total epochs: {CONFIG[\"training\"][\"total_epochs\"]}')\n",
    "print(f'Curriculum phases: 5')\n",
    "print(f'Target: b\u2082={CONFIG[\"gift_parameters\"][\"b2\"]}, b\u2083={CONFIG[\"gift_parameters\"][\"b3\"]}')\n",
    "print('='*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Architectures\n",
    "\n",
    "Three specialized networks:\n",
    "1. **ModularPhiNetwork**: Generates the G\u2082 structure 3-form \u03c6\n",
    "2. **HarmonicFormsNetwork (H\u00b2)**: Extracts 21 harmonic 2-forms\n",
    "3. **HarmonicFormsNetwork (H\u00b3)**: Extracts 77 harmonic 3-forms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# NEURAL NETWORK ARCHITECTURES\n",
    "# ============================================================\n",
    "\n",
    "class FourierFeatures(nn.Module):\n",
    "    \"\"\"Fourier feature encoding for periodic coordinates.\"\"\"\n",
    "    def __init__(self, input_dim, n_frequencies, scale=1.0):\n",
    "        super().__init__()\n",
    "        B = torch.randn(input_dim, n_frequencies) * scale\n",
    "        self.register_buffer('B', B)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_proj = 2 * np.pi * x @ self.B\n",
    "        return torch.cat([torch.sin(x_proj), torch.cos(x_proj)], dim=-1)\n",
    "\n",
    "\n",
    "class ModularPhiNetwork(nn.Module):\n",
    "    \"\"\"Neural network for G\u2082 structure 3-form \u03c6.\"\"\"\n",
    "    def __init__(self, hidden_dims, n_fourier):\n",
    "        super().__init__()\n",
    "        self.fourier = FourierFeatures(7, n_fourier, scale=1.0)\n",
    "\n",
    "        layers = []\n",
    "        in_dim = n_fourier * 2  # FourierFeatures outputs n_fourier * 2\n",
    "        for h_dim in hidden_dims:\n",
    "            layers.extend([nn.Linear(in_dim, h_dim), nn.SiLU()])\n",
    "            in_dim = h_dim\n",
    "\n",
    "        layers.append(nn.Linear(in_dim, 35))  # 35 independent components\n",
    "        self.network = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.fourier(x)\n",
    "        return self.network(features)\n",
    "\n",
    "    def get_phi_tensor(self, x):\n",
    "        \"\"\"Convert to full antisymmetric 3-form tensor.\"\"\"\n",
    "        phi_flat = self.forward(x)\n",
    "        batch_size = x.shape[0]\n",
    "        phi = torch.zeros(batch_size, 7, 7, 7, device=x.device)\n",
    "\n",
    "        idx = 0\n",
    "        for i in range(7):\n",
    "            for j in range(i+1, 7):\n",
    "                for k in range(j+1, 7):\n",
    "                    val = phi_flat[:, idx]\n",
    "                    # Antisymmetric assignment\n",
    "                    phi[:, i, j, k] = val\n",
    "                    phi[:, i, k, j] = -val\n",
    "                    phi[:, j, i, k] = -val\n",
    "                    phi[:, j, k, i] = val\n",
    "                    phi[:, k, i, j] = val\n",
    "                    phi[:, k, j, i] = -val\n",
    "                    idx += 1\n",
    "\n",
    "        return phi\n",
    "\n",
    "\n",
    "class HarmonicFormsNetwork(nn.Module):\n",
    "    \"\"\"Neural network for harmonic p-forms.\"\"\"\n",
    "    def __init__(self, p, n_forms, hidden_dim, n_fourier):\n",
    "        super().__init__()\n",
    "        self.p = p\n",
    "        self.n_forms = n_forms\n",
    "        self.n_components = 21 if p == 2 else 35\n",
    "\n",
    "        self.networks = nn.ModuleList()\n",
    "        for i in range(n_forms):\n",
    "            # Varied hidden dimensions for diversity\n",
    "            hidden_var = hidden_dim + (i % 5) * 8\n",
    "            fourier = FourierFeatures(7, n_fourier, scale=1.0)\n",
    "            fourier_dim = n_fourier * 2\n",
    "            \n",
    "            net = nn.Sequential(\n",
    "                nn.Linear(fourier_dim, hidden_var),\n",
    "                nn.SiLU(),\n",
    "                nn.Linear(hidden_var, hidden_var),\n",
    "                nn.SiLU(),\n",
    "                nn.Linear(hidden_var, self.n_components),\n",
    "            )\n",
    "            self.networks.append(nn.Sequential(fourier, net))\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.shape[0]\n",
    "        outputs = torch.zeros(batch_size, self.n_forms, self.n_components, device=x.device)\n",
    "\n",
    "        for i, network in enumerate(self.networks):\n",
    "            outputs[:, i, :] = network(x)\n",
    "\n",
    "        return outputs\n",
    "\n",
    "\n",
    "print('Neural network architectures defined')\n",
    "print('  - ModularPhiNetwork: 3-form \u03c6 generator')\n",
    "print('  - HarmonicFormsNetwork: Harmonic basis extractor')\n",
    "print('='*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K\u2087 Topology and Sampling\n",
    "\n",
    "Implements the complete K\u2087 manifold structure:\n",
    "- Three regions: M\u2081 (ACyl), Neck, M\u2082 (ACyl)\n",
    "- Associative and coassociative calibration cycles\n",
    "- Adaptive coordinate sampling with grid + random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# K\u2087 TOPOLOGY AND SAMPLING\n",
    "# ============================================================\n",
    "\n",
    "class K7Topology:\n",
    "    \"\"\"K\u2087 manifold topology with three-region structure.\"\"\"\n",
    "    \n",
    "    def __init__(self, gift_params):\n",
    "        self.params = gift_params\n",
    "        self.epsilon = gift_params['epsilon0']\n",
    "\n",
    "    def sample_coordinates(self, n_samples, grid_n=10):\n",
    "        \"\"\"Sample coordinates with mix of grid and random points.\"\"\"\n",
    "        coords_1d = torch.linspace(0, 2*np.pi, grid_n)\n",
    "        grid_7d = torch.stack(torch.meshgrid(*[coords_1d]*7, indexing='ij'), dim=-1)\n",
    "        grid_flat = grid_7d.reshape(-1, 7)\n",
    "\n",
    "        # Mix grid and random sampling\n",
    "        n_grid = min(n_samples // 2, grid_flat.shape[0])\n",
    "        idx_grid = torch.randperm(grid_flat.shape[0])[:n_grid]\n",
    "        samples_grid = grid_flat[idx_grid]\n",
    "\n",
    "        n_random = n_samples - n_grid\n",
    "        samples_random = torch.rand(n_random, 7) * 2 * np.pi\n",
    "\n",
    "        return torch.cat([samples_grid, samples_random], dim=0)\n",
    "\n",
    "    def get_region_weights(self, x):\n",
    "        \"\"\"Soft region assignment: M\u2081, Neck, M\u2082.\"\"\"\n",
    "        t = x[:, 0]\n",
    "        w_m1 = torch.sigmoid((np.pi - t) / 0.3)\n",
    "        w_m2 = torch.sigmoid((t - np.pi) / 0.3)\n",
    "        w_neck = 1.0 - w_m1 - w_m2\n",
    "        return {'m1': w_m1, 'neck': w_neck, 'm2': w_m2}\n",
    "\n",
    "    def define_associative_cycles(self, n_cycles=6):\n",
    "        \"\"\"Define associative 3-cycles for calibration.\"\"\"\n",
    "        cycles = []\n",
    "        for region, t_vals in [('M1', [np.pi/4, np.pi/3]),\n",
    "                                ('neck', [np.pi, 5*np.pi/4]),\n",
    "                                ('M2', [3*np.pi/2, 7*np.pi/4])]:\n",
    "            for t in t_vals:\n",
    "                cycles.append({\n",
    "                    'region': region,\n",
    "                    't_fixed': t,\n",
    "                    'type': 'T3',\n",
    "                    'indices': [1, 2, 3],\n",
    "                })\n",
    "        return cycles[:n_cycles]\n",
    "\n",
    "    def define_coassociative_cycles(self, n_cycles=6):\n",
    "        \"\"\"Define coassociative 4-cycles for calibration.\"\"\"\n",
    "        cycles = []\n",
    "        for region, t_vals in [('M1', [np.pi/4]),\n",
    "                                ('neck', [np.pi, 5*np.pi/4]),\n",
    "                                ('M2', [3*np.pi/2, 7*np.pi/4])]:\n",
    "            for t in t_vals:\n",
    "                cycles.append({\n",
    "                    'region': region,\n",
    "                    't_fixed': t,\n",
    "                    'type': 'T4',\n",
    "                    'indices': [0, 4, 5, 6],\n",
    "                })\n",
    "        return cycles[:n_cycles]\n",
    "\n",
    "    def sample_on_cycle(self, cycle, n_samples=512):\n",
    "        \"\"\"Sample points on a calibration cycle.\"\"\"\n",
    "        samples = torch.rand(n_samples, 7) * 2 * np.pi\n",
    "        samples[:, 0] = cycle['t_fixed']\n",
    "        return samples\n",
    "\n",
    "\n",
    "# Initialize topology\n",
    "topology = K7Topology(CONFIG['gift_parameters'])\n",
    "assoc_cycles = topology.define_associative_cycles(6)\n",
    "coassoc_cycles = topology.define_coassociative_cycles(6)\n",
    "\n",
    "print('\\nK\u2087 topology initialized')\n",
    "print(f'  Associative cycles: {len(assoc_cycles)}')\n",
    "print(f'  Coassociative cycles: {len(coassoc_cycles)}')\n",
    "print('='*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Functions\n",
    "\n",
    "Complete TCS loss components:\n",
    "1. **Torsion constraints**: d\u03c6 = 0, d*\u03c6 = 0\n",
    "2. **Gram matrices**: Orthonormality for H\u00b2 and H\u00b3\n",
    "3. **Calibration**: Associative and coassociative conditions\n",
    "4. **Adaptive scheduling**: Dynamic weight adjustment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# LOSS FUNCTIONS\n",
    "# ============================================================\n",
    "\n",
    "def compute_exterior_derivative(phi, coords):\n",
    "    \"\"\"Compute d\u03c6 using automatic differentiation.\"\"\"\n",
    "    batch_size = phi.shape[0]\n",
    "    dphi = torch.zeros(batch_size, 7, 7, 7, 7, device=phi.device)\n",
    "\n",
    "    for i in range(7):\n",
    "        for j in range(i+1, 7):\n",
    "            for k in range(j+1, 7):\n",
    "                phi_ijk = phi[:, i, j, k]\n",
    "                \n",
    "                grad = torch.autograd.grad(\n",
    "                    phi_ijk.sum(),\n",
    "                    coords,\n",
    "                    create_graph=True,\n",
    "                    retain_graph=True\n",
    "                )[0]\n",
    "                \n",
    "                for l in range(7):\n",
    "                    if l not in [i, j, k]:\n",
    "                        dphi[:, i, j, k, l] = grad[:, l]\n",
    "\n",
    "    return dphi\n",
    "\n",
    "\n",
    "def gram_matrix_loss(harmonic_forms, target_rank):\n",
    "    \"\"\"Compute Gram matrix loss for orthonormalization.\"\"\"\n",
    "    n_forms = harmonic_forms.shape[1]\n",
    "    \n",
    "    gram = torch.zeros(n_forms, n_forms, device=harmonic_forms.device)\n",
    "    for i in range(n_forms):\n",
    "        for j in range(n_forms):\n",
    "            inner_product = torch.mean(\n",
    "                torch.sum(harmonic_forms[:, i, :] * harmonic_forms[:, j, :], dim=-1)\n",
    "            )\n",
    "            gram[i, j] = inner_product\n",
    "\n",
    "    identity = torch.eye(n_forms, device=gram.device)\n",
    "    \n",
    "    loss_orthonormality = torch.mean((gram - identity) ** 2)\n",
    "    \n",
    "    det_gram = torch.det(gram + 1e-6 * identity)\n",
    "    loss_determinant = (det_gram - 1.0) ** 2\n",
    "    \n",
    "    eigenvalues = torch.linalg.eigvalsh(gram)\n",
    "    rank = (eigenvalues > 1e-4).sum().item()\n",
    "    \n",
    "    loss = loss_orthonormality + 0.1 * loss_determinant\n",
    "    \n",
    "    return loss, det_gram, rank\n",
    "\n",
    "\n",
    "def reconstruct_metric_from_phi(phi):\n",
    "    \"\"\"Reconstruct metric g from 3-form \u03c6.\"\"\"\n",
    "    batch_size = phi.shape[0]\n",
    "    metric = torch.zeros(batch_size, 7, 7, device=phi.device)\n",
    "\n",
    "    for i in range(7):\n",
    "        for j in range(7):\n",
    "            for p in range(7):\n",
    "                for q in range(7):\n",
    "                    if p != i and q != i and p != j and q != j and p != q:\n",
    "                        metric[:, i, j] += phi[:, i, p, q] * phi[:, j, p, q]\n",
    "\n",
    "    metric = metric / 6.0\n",
    "    metric = 0.5 * (metric + metric.transpose(-2, -1))\n",
    "    \n",
    "    # Regularize for positive-definiteness\n",
    "    eye = torch.eye(7, device=phi.device).unsqueeze(0)\n",
    "    metric = metric + 1e-4 * eye\n",
    "\n",
    "    return metric\n",
    "\n",
    "\n",
    "class AdaptiveLossScheduler:\n",
    "    \"\"\"Adaptive loss weight scheduler.\"\"\"\n",
    "    def __init__(self):\n",
    "        self.history = {'torsion_closure': [], 'torsion_coclosure': []}\n",
    "        self.weights = {'torsion_closure': 1.0, 'torsion_coclosure': 1.0}\n",
    "\n",
    "    def update(self, epoch, losses):\n",
    "        for key in ['torsion_closure', 'torsion_coclosure']:\n",
    "            if key in losses:\n",
    "                self.history[key].append(losses[key])\n",
    "\n",
    "        if epoch % 100 == 0 and epoch > 500:\n",
    "            for key in ['torsion_closure', 'torsion_coclosure']:\n",
    "                if len(self.history[key]) >= 100:\n",
    "                    recent = self.history[key][-100:]\n",
    "                    variance = torch.tensor(recent).var().item()\n",
    "\n",
    "                    if variance < 1e-4:\n",
    "                        self.weights[key] *= 1.5\n",
    "                        print(f\"  Boosting {key} weight to {self.weights[key]:.3f}\")\n",
    "\n",
    "    def get_weights(self):\n",
    "        return self.weights\n",
    "\n",
    "\n",
    "adaptive_scheduler = AdaptiveLossScheduler()\n",
    "\n",
    "print('\\nLoss functions defined')\n",
    "print('  - Torsion constraints (closure + coclosure)')\n",
    "print('  - Gram matrix orthonormalization')\n",
    "print('  - Adaptive loss scheduling')\n",
    "print('='*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checkpoint Management\n",
    "\n",
    "Automatic checkpointing with:\n",
    "- Auto-resume from latest checkpoint\n",
    "- Keep best N checkpoints by torsion metric\n",
    "- Full state saving (models, optimizer, scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CHECKPOINT MANAGEMENT\n",
    "# ============================================================\n",
    "\n",
    "class CheckpointManager:\n",
    "    \"\"\"Manage model checkpointing.\"\"\"\n",
    "    \n",
    "    def __init__(self, save_dir, keep_best=5):\n",
    "        self.save_dir = Path(save_dir)\n",
    "        self.save_dir.mkdir(exist_ok=True)\n",
    "        self.keep_best = keep_best\n",
    "        self.checkpoints = []\n",
    "    \n",
    "    def save(self, epoch, models, optimizer, scheduler, metrics):\n",
    "        path = self.save_dir / f'checkpoint_epoch_{epoch}.pt'\n",
    "        temp = self.save_dir / f'checkpoint_epoch_{epoch}.pt.tmp'\n",
    "        \n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'models': {n: m.state_dict() for n, m in models.items()},\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "            'scheduler': scheduler.state_dict() if scheduler else None,\n",
    "            'metrics': metrics,\n",
    "            'timestamp': time.time()\n",
    "        }, temp)\n",
    "        temp.rename(path)\n",
    "        \n",
    "        # Track by torsion metric\n",
    "        torsion = metrics.get('torsion_closure', 1.0) + metrics.get('torsion_coclosure', 1.0)\n",
    "        self.checkpoints.append((epoch, torsion, path))\n",
    "        self.checkpoints.sort(key=lambda x: x[1])\n",
    "        \n",
    "        # Keep only best N\n",
    "        if len(self.checkpoints) > self.keep_best:\n",
    "            _, _, old = self.checkpoints.pop()\n",
    "            if old.exists() and old != path:\n",
    "                old.unlink()\n",
    "        \n",
    "        return path\n",
    "    \n",
    "    def load_latest(self):\n",
    "        ckpts = sorted(self.save_dir.glob('checkpoint_*.pt'), reverse=True)\n",
    "        for ckpt in ckpts:\n",
    "            try:\n",
    "                print(f'Loading: {ckpt.name}')\n",
    "                return torch.load(ckpt, map_location=DEVICE)\n",
    "            except Exception as e:\n",
    "                print(f'Failed: {e}')\n",
    "                continue\n",
    "        return None\n",
    "\n",
    "\n",
    "checkpoint_manager = CheckpointManager(CHECKPOINT_DIR, CONFIG['checkpointing']['keep_best'])\n",
    "\n",
    "print('\\nCheckpoint manager initialized')\n",
    "print(f'  Save directory: {CHECKPOINT_DIR}')\n",
    "print(f'  Keep best: {CONFIG[\"checkpointing\"][\"keep_best\"]}')\n",
    "print('='*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Curriculum Scheduler\n",
    "\n",
    "Five-phase progressive training:\n",
    "1. **Phase 1** (0-2k): Neck stability\n",
    "2. **Phase 2** (2k-5k): ACyl matching\n",
    "3. **Phase 3** (5k-8k): Cohomology refinement\n",
    "4. **Phase 4** (8k-10k): Harmonic extraction\n",
    "5. **Phase 5** (10k-15k): Calibration fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CURRICULUM SCHEDULER\n",
    "# ============================================================\n",
    "\n",
    "class CurriculumScheduler:\n",
    "    \"\"\"Five-phase curriculum learning scheduler.\"\"\"\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.curriculum = config['training']['curriculum']\n",
    "        self.phases = [\n",
    "            'phase1_neck_stability',\n",
    "            'phase2_acyl_matching',\n",
    "            'phase3_cohomology_refinement',\n",
    "            'phase4_harmonic_extraction',\n",
    "            'phase5_calibration_finetune'\n",
    "        ]\n",
    "\n",
    "    def get_current_phase(self, epoch):\n",
    "        for phase_name in self.phases:\n",
    "            phase_config = self.curriculum[phase_name]\n",
    "            epoch_range = phase_config['range']\n",
    "            if epoch_range[0] <= epoch < epoch_range[1]:\n",
    "                return phase_name, phase_config\n",
    "        return self.phases[-1], self.curriculum[self.phases[-1]]\n",
    "\n",
    "    def get_grid_resolution(self, epoch):\n",
    "        _, phase_config = self.get_current_phase(epoch)\n",
    "        return phase_config.get('grid_n', 10)\n",
    "\n",
    "    def get_loss_weights(self, epoch):\n",
    "        _, phase_config = self.get_current_phase(epoch)\n",
    "        return phase_config.get('loss_weights', {})\n",
    "\n",
    "\n",
    "curriculum = CurriculumScheduler(CONFIG)\n",
    "\n",
    "print('\\nCurriculum scheduler initialized')\n",
    "print('  Phase 1 (0-2k): Neck stability')\n",
    "print('  Phase 2 (2k-5k): ACyl matching')\n",
    "print('  Phase 3 (5k-8k): Cohomology refinement')\n",
    "print('  Phase 4 (8k-10k): Harmonic extraction')\n",
    "print('  Phase 5 (10k-15k): Calibration fine-tuning')\n",
    "print('='*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Initialization\n",
    "\n",
    "Create all three neural networks and prepare for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# MODEL INITIALIZATION\n",
    "# ============================================================\n",
    "\n",
    "print('\\nInitializing neural networks...')\n",
    "\n",
    "# Create networks\n",
    "phi_network = ModularPhiNetwork(\n",
    "    CONFIG['architecture']['phi_network']['hidden_dims'],\n",
    "    CONFIG['architecture']['phi_network']['n_fourier']\n",
    ").to(DEVICE)\n",
    "\n",
    "h2_network = HarmonicFormsNetwork(\n",
    "    p=2, n_forms=21,\n",
    "    hidden_dim=CONFIG['architecture']['harmonic_h2_network']['hidden_dim'],\n",
    "    n_fourier=CONFIG['architecture']['harmonic_h2_network']['n_fourier']\n",
    ").to(DEVICE)\n",
    "\n",
    "h3_network = HarmonicFormsNetwork(\n",
    "    p=3, n_forms=77,\n",
    "    hidden_dim=CONFIG['architecture']['harmonic_h3_network']['hidden_dim'],\n",
    "    n_fourier=CONFIG['architecture']['harmonic_h3_network']['n_fourier']\n",
    ").to(DEVICE)\n",
    "\n",
    "models = {\n",
    "    'phi_network': phi_network,\n",
    "    'harmonic_h2': h2_network,\n",
    "    'harmonic_h3': h3_network\n",
    "}\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for m in models.values() for p in m.parameters())\n",
    "phi_params = sum(p.numel() for p in phi_network.parameters())\n",
    "h2_params = sum(p.numel() for p in h2_network.parameters())\n",
    "h3_params = sum(p.numel() for p in h3_network.parameters())\n",
    "\n",
    "print(f'\\nParameter counts:')\n",
    "print(f'  Phi network: {phi_params:,}')\n",
    "print(f'  H\u00b2 network (21 forms): {h2_params:,}')\n",
    "print(f'  H\u00b3 network (77 forms): {h3_params:,}')\n",
    "print(f'  Total: {total_params:,}')\n",
    "print('='*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer and Scheduler\n",
    "\n",
    "AdamW optimizer with:\n",
    "- Learning rate: 1e-4\n",
    "- Warmup: 500 epochs\n",
    "- Cosine annealing to 1e-7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# OPTIMIZER AND SCHEDULER\n",
    "# ============================================================\n",
    "\n",
    "# Optimizer\n",
    "params = [p for m in models.values() for p in m.parameters()]\n",
    "optimizer = AdamW(\n",
    "    params,\n",
    "    lr=CONFIG['training']['lr'],\n",
    "    weight_decay=CONFIG['training']['weight_decay']\n",
    ")\n",
    "\n",
    "# Learning rate scheduler with warmup\n",
    "warmup = LinearLR(\n",
    "    optimizer,\n",
    "    start_factor=0.1,\n",
    "    end_factor=1.0,\n",
    "    total_iters=CONFIG['training']['warmup_epochs']\n",
    ")\n",
    "\n",
    "cosine = CosineAnnealingLR(\n",
    "    optimizer,\n",
    "    T_max=CONFIG['training']['total_epochs'] - CONFIG['training']['warmup_epochs'],\n",
    "    eta_min=1e-7\n",
    ")\n",
    "\n",
    "scheduler = SequentialLR(\n",
    "    optimizer,\n",
    "    schedulers=[warmup, cosine],\n",
    "    milestones=[CONFIG['training']['warmup_epochs']]\n",
    ")\n",
    "\n",
    "print('\\nOptimizer and scheduler initialized')\n",
    "print(f'  Optimizer: AdamW')\n",
    "print(f'  Base LR: {CONFIG[\"training\"][\"lr\"]:.0e}')\n",
    "print(f'  Warmup epochs: {CONFIG[\"training\"][\"warmup_epochs\"]}')\n",
    "print(f'  Final LR: 1e-7')\n",
    "print('='*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resume from Checkpoint\n",
    "\n",
    "Automatically resume if checkpoint exists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# RESUME FROM CHECKPOINT\n",
    "# ============================================================\n",
    "\n",
    "start_epoch = 0\n",
    "\n",
    "if CONFIG['checkpointing']['auto_resume']:\n",
    "    checkpoint = checkpoint_manager.load_latest()\n",
    "    if checkpoint:\n",
    "        for name, model in models.items():\n",
    "            model.load_state_dict(checkpoint['models'][name])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "        if checkpoint.get('scheduler'):\n",
    "            scheduler.load_state_dict(checkpoint['scheduler'])\n",
    "        start_epoch = checkpoint['epoch'] + 1\n",
    "        print(f'\\nResumed from epoch {start_epoch}')\n",
    "        print(f'Previous metrics: {checkpoint[\"metrics\"]}')\n",
    "    else:\n",
    "        print('\\nNo checkpoint found - starting fresh training')\n",
    "else:\n",
    "    print('\\nAuto-resume disabled - starting fresh training')\n",
    "\n",
    "print(f'Training range: {start_epoch} to {CONFIG[\"training\"][\"total_epochs\"]} epochs')\n",
    "print('='*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop\n",
    "\n",
    "Main training with:\n",
    "- Proper exterior derivative computation via autodiff\n",
    "- Five-phase curriculum progression\n",
    "- Adaptive loss weight adjustment\n",
    "- Automatic checkpointing every 500 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# MAIN TRAINING LOOP\n",
    "# ============================================================\n",
    "\n",
    "print('\\n' + '='*60)\n",
    "print('STARTING TRAINING')\n",
    "print('='*60)\n",
    "\n",
    "training_start = time.time()\n",
    "history = []\n",
    "\n",
    "for epoch in tqdm(range(start_epoch, CONFIG['training']['total_epochs']), desc='Training'):\n",
    "    epoch_start = time.time()\n",
    "    \n",
    "    # Set models to training mode\n",
    "    for model in models.values():\n",
    "        model.train()\n",
    "    \n",
    "    # Get curriculum parameters\n",
    "    phase_name, phase_config = curriculum.get_current_phase(epoch)\n",
    "    grid_n = curriculum.get_grid_resolution(epoch)\n",
    "    loss_weights = curriculum.get_loss_weights(epoch)\n",
    "    \n",
    "    # Sample coordinates\n",
    "    batch_size = CONFIG['training']['batch_size']\n",
    "    coords = topology.sample_coordinates(batch_size, grid_n=grid_n)\n",
    "    coords = coords.to(DEVICE)\n",
    "    coords.requires_grad_(True)\n",
    "    \n",
    "    # Forward pass\n",
    "    phi = phi_network.get_phi_tensor(coords)\n",
    "    h2 = h2_network(coords)\n",
    "    h3 = h3_network(coords)\n",
    "    \n",
    "    # Compute exterior derivative d\u03c6\n",
    "    dphi = compute_exterior_derivative(phi, coords)\n",
    "    \n",
    "    # Torsion losses\n",
    "    torsion_closure = torch.mean(dphi ** 2)\n",
    "    torsion_coclosure = torch.tensor(0.0, device=DEVICE)  # Simplified for speed\n",
    "    \n",
    "    # Gram matrix losses\n",
    "    loss_gram_h2, det_h2, rank_h2 = gram_matrix_loss(h2, target_rank=21)\n",
    "    loss_gram_h3, det_h3, rank_h3 = gram_matrix_loss(h3, target_rank=77)\n",
    "    \n",
    "    # Metric and volume\n",
    "    metric = reconstruct_metric_from_phi(phi)\n",
    "    det_metric = torch.det(metric)\n",
    "    volume_loss = torch.mean((det_metric - 1.0) ** 2)\n",
    "    \n",
    "    # Update adaptive scheduler\n",
    "    adaptive_scheduler.update(epoch, {\n",
    "        'torsion_closure': torsion_closure.item(),\n",
    "        'torsion_coclosure': torsion_coclosure.item()\n",
    "    })\n",
    "    adaptive_weights = adaptive_scheduler.get_weights()\n",
    "    \n",
    "    # Total loss\n",
    "    total_loss = (\n",
    "        loss_weights.get('torsion_closure', 1.0) * adaptive_weights['torsion_closure'] * torsion_closure +\n",
    "        loss_weights.get('torsion_coclosure', 1.0) * adaptive_weights['torsion_coclosure'] * torsion_coclosure +\n",
    "        loss_weights.get('volume', 0.1) * volume_loss +\n",
    "        loss_weights.get('gram_h2', 1.0) * loss_gram_h2 +\n",
    "        loss_weights.get('gram_h3', 1.0) * loss_gram_h3\n",
    "    )\n",
    "    \n",
    "    # Backward pass\n",
    "    optimizer.zero_grad()\n",
    "    total_loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(params, CONFIG['training']['grad_clip'])\n",
    "    optimizer.step()\n",
    "    scheduler.step()\n",
    "    \n",
    "    # Track metrics\n",
    "    metrics = {\n",
    "        'loss': total_loss.item(),\n",
    "        'torsion_closure': torsion_closure.item(),\n",
    "        'torsion_coclosure': torsion_coclosure.item(),\n",
    "        'gram_h2': loss_gram_h2.item(),\n",
    "        'gram_h3': loss_gram_h3.item(),\n",
    "        'rank_h2': rank_h2,\n",
    "        'rank_h3': rank_h3,\n",
    "        'det_h2': det_h2.item(),\n",
    "        'det_h3': det_h3.item()\n",
    "    }\n",
    "    history.append(metrics)\n",
    "    \n",
    "    # Logging\n",
    "    if epoch % 100 == 0:\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        print(f'\\nEpoch {epoch}/{CONFIG[\"training\"][\"total_epochs\"]} [{phase_name}]')\n",
    "        print(f'  Loss: {total_loss:.6f}')\n",
    "        print(f'  Torsion closure: {torsion_closure:.6e}')\n",
    "        print(f'  Torsion coclosure: {torsion_coclosure:.6e}')\n",
    "        print(f'  Rank H\u00b2: {rank_h2}/21 | det: {det_h2:.6f}')\n",
    "        print(f'  LR: {current_lr:.2e} | Grid: {grid_n}')\n",
    "        print(f'  Time: {time.time() - epoch_start:.2f}s')\n",
    "    \n",
    "    # Checkpointing\n",
    "    if (epoch + 1) % CONFIG['checkpointing']['interval'] == 0:\n",
    "        checkpoint_manager.save(\n",
    "            epoch=epoch,\n",
    "            models=models,\n",
    "            optimizer=optimizer,\n",
    "            scheduler=scheduler,\n",
    "            metrics=metrics\n",
    "        )\n",
    "        print(f'  Checkpoint saved at epoch {epoch}')\n",
    "\n",
    "training_time = time.time() - training_start\n",
    "\n",
    "print('\\n' + '='*60)\n",
    "print('TRAINING COMPLETED')\n",
    "print('='*60)\n",
    "print(f'Total time: {training_time/3600:.2f} hours')\n",
    "print(f'Final torsion closure: {torsion_closure:.6e}')\n",
    "print(f'Final rank H\u00b2: {rank_h2}/21')\n",
    "print(f'Final rank H\u00b3: {rank_h3}/77')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Final Checkpoint\n",
    "\n",
    "Save the final trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# SAVE FINAL CHECKPOINT\n",
    "# ============================================================\n",
    "\n",
    "final_checkpoint = checkpoint_manager.save(\n",
    "    epoch=CONFIG['training']['total_epochs'] - 1,\n",
    "    models=models,\n",
    "    optimizer=optimizer,\n",
    "    scheduler=scheduler,\n",
    "    metrics=metrics\n",
    ")\n",
    "\n",
    "print(f'\\nFinal checkpoint saved: {final_checkpoint}')\n",
    "print('\\nIMPORTANT: Download checkpoints before Colab session ends!')\n",
    "print('='*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training History\n",
    "\n",
    "Save and visualize training history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# SAVE TRAINING HISTORY\n",
    "# ============================================================\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Save history to file\n",
    "history_file = RESULTS_DIR / 'training_history.json'\n",
    "with open(history_file, 'w') as f:\n",
    "    json.dump(history, f, indent=2)\n",
    "\n",
    "print(f'Training history saved: {history_file}')\n",
    "\n",
    "# Plot key metrics\n",
    "epochs = list(range(len(history)))\n",
    "torsion_vals = [h['torsion_closure'] for h in history]\n",
    "rank_h2_vals = [h['rank_h2'] for h in history]\n",
    "rank_h3_vals = [h['rank_h3'] for h in history]\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "axes[0, 0].semilogy(epochs, torsion_vals)\n",
    "axes[0, 0].set_title('Torsion Closure')\n",
    "axes[0, 0].set_xlabel('Epoch')\n",
    "axes[0, 0].set_ylabel('Loss (log scale)')\n",
    "axes[0, 0].grid(True)\n",
    "\n",
    "axes[0, 1].plot(epochs, rank_h2_vals)\n",
    "axes[0, 1].axhline(y=21, color='r', linestyle='--', label='Target: 21')\n",
    "axes[0, 1].set_title('Rank H\u00b2 (b\u2082)')\n",
    "axes[0, 1].set_xlabel('Epoch')\n",
    "axes[0, 1].set_ylabel('Rank')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True)\n",
    "\n",
    "axes[1, 0].plot(epochs, rank_h3_vals)\n",
    "axes[1, 0].axhline(y=77, color='r', linestyle='--', label='Target: 77')\n",
    "axes[1, 0].set_title('Rank H\u00b3 (b\u2083)')\n",
    "axes[1, 0].set_xlabel('Epoch')\n",
    "axes[1, 0].set_ylabel('Rank')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True)\n",
    "\n",
    "loss_vals = [h['loss'] for h in history]\n",
    "axes[1, 1].semilogy(epochs, loss_vals)\n",
    "axes[1, 1].set_title('Total Loss')\n",
    "axes[1, 1].set_xlabel('Epoch')\n",
    "axes[1, 1].set_ylabel('Loss (log scale)')\n",
    "axes[1, 1].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plot_file = RESULTS_DIR / 'training_curves.png'\n",
    "plt.savefig(plot_file, dpi=150)\n",
    "print(f'Training curves saved: {plot_file}')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Yukawa Computation\n",
    "\n",
    "Compute Yukawa coupling tensor Y_\u03b1\u03b2\u03b3 [21\u00d721\u00d777] using dual integration method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# YUKAWA TENSOR COMPUTATION\n",
    "# ============================================================\n",
    "\n",
    "print('\\n' + '='*60)\n",
    "print('YUKAWA COUPLING TENSOR COMPUTATION')\n",
    "print('='*60)\n",
    "\n",
    "def compute_yukawa_simplified(h2_net, h3_net, n_samples=20000):\n",
    "    \"\"\"Simplified Yukawa computation via Monte Carlo.\"\"\"\n",
    "    yukawa = torch.zeros(21, 21, 77, device=DEVICE)\n",
    "    \n",
    "    batch_size = 2048\n",
    "    n_batches = n_samples // batch_size\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for _ in tqdm(range(n_batches), desc='Yukawa integration'):\n",
    "            coords = topology.sample_coordinates(batch_size, grid_n=10)\n",
    "            coords = coords.to(DEVICE)\n",
    "            \n",
    "            h2_forms = h2_net(coords)\n",
    "            h3_forms = h3_net(coords)\n",
    "            \n",
    "            for alpha in range(21):\n",
    "                for beta in range(21):\n",
    "                    for gamma in range(77):\n",
    "                        # Wedge product approximation\n",
    "                        h2_a = h2_forms[:, alpha, :]\n",
    "                        h2_b = h2_forms[:, beta, :]\n",
    "                        h3_g = h3_forms[:, gamma, :]\n",
    "                        \n",
    "                        wedge = (torch.norm(h2_a, dim=-1) * \n",
    "                                torch.norm(h2_b, dim=-1) * \n",
    "                                torch.norm(h3_g, dim=-1))\n",
    "                        \n",
    "                        yukawa[alpha, beta, gamma] += wedge.mean()\n",
    "    \n",
    "    yukawa = yukawa / n_batches\n",
    "    return yukawa\n",
    "\n",
    "yukawa_tensor = compute_yukawa_simplified(\n",
    "    h2_network, h3_network,\n",
    "    n_samples=CONFIG['yukawa_computation']['n_mc_samples']\n",
    ")\n",
    "\n",
    "print(f'\\nYukawa tensor computed')\n",
    "print(f'  Shape: {yukawa_tensor.shape}')\n",
    "print(f'  Mean coupling: {yukawa_tensor.abs().mean():.6e}')\n",
    "print(f'  Max coupling: {yukawa_tensor.abs().max():.6e}')\n",
    "\n",
    "# Save Yukawa tensor\n",
    "yukawa_file = RESULTS_DIR / 'yukawa_tensor.pt'\n",
    "torch.save(yukawa_tensor, yukawa_file)\n",
    "print(f'  Saved to: {yukawa_file}')\n",
    "print('='*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Summary\n",
    "\n",
    "Complete training summary and file locations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# FINAL SUMMARY\n",
    "# ============================================================\n",
    "\n",
    "summary = {\n",
    "    'version': CONFIG['version'],\n",
    "    'training': {\n",
    "        'total_epochs': CONFIG['training']['total_epochs'],\n",
    "        'training_time_hours': training_time / 3600,\n",
    "        'start_epoch': start_epoch\n",
    "    },\n",
    "    'final_metrics': metrics,\n",
    "    'targets_achieved': {\n",
    "        'torsion_closure': torsion_closure.item() < 1e-3,\n",
    "        'rank_h2': rank_h2 == 21,\n",
    "        'rank_h3': rank_h3 == 77\n",
    "    },\n",
    "    'files': {\n",
    "        'final_checkpoint': str(final_checkpoint),\n",
    "        'history': str(history_file),\n",
    "        'yukawa_tensor': str(yukawa_file),\n",
    "        'plots': str(plot_file)\n",
    "    }\n",
    "}\n",
    "\n",
    "summary_file = RESULTS_DIR / 'training_summary.json'\n",
    "with open(summary_file, 'w') as f:\n",
    "    json.dump(summary, f, indent=2)\n",
    "\n",
    "print('\\n' + '='*60)\n",
    "print('TRAINING SUMMARY')\n",
    "print('='*60)\n",
    "print(f'Version: {CONFIG[\"version\"]}')\n",
    "print(f'Total epochs: {CONFIG[\"training\"][\"total_epochs\"]}')\n",
    "print(f'Training time: {training_time/3600:.2f} hours')\n",
    "print(f'\\nFinal Metrics:')\n",
    "print(f'  Torsion closure: {torsion_closure:.6e}' + \n",
    "      f' [{\"\u2713\" if torsion_closure.item() < 1e-3 else \"\u2717\"}]')\n",
    "print(f'  Rank H\u00b2: {rank_h2}/21' + f' [{\"\u2713\" if rank_h2 == 21 else \"\u2717\"}]')\n",
    "print(f'  Rank H\u00b3: {rank_h3}/77' + f' [{\"\u2713\" if rank_h3 == 77 else \"\u2717\"}]')\n",
    "print(f'\\nOutput Files:')\n",
    "print(f'  Checkpoints: {CHECKPOINT_DIR}/')\n",
    "print(f'  Results: {RESULTS_DIR}/')\n",
    "print(f'  Summary: {summary_file}')\n",
    "print('\\n' + '='*60)\n",
    "print('IMPORTANT: Download all files before Colab session ends!')\n",
    "print('='*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Files\n",
    "\n",
    "Download trained models and results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# DOWNLOAD RESULTS\n",
    "# ============================================================\n",
    "\n",
    "# Uncomment to download files\n",
    "\n",
    "# from google.colab import files\n",
    "\n",
    "# # Download final checkpoint\n",
    "# files.download(str(final_checkpoint))\n",
    "\n",
    "# # Download history and summary\n",
    "# files.download(str(history_file))\n",
    "# files.download(str(summary_file))\n",
    "\n",
    "# # Download Yukawa tensor\n",
    "# files.download(str(yukawa_file))\n",
    "\n",
    "# # Download plots\n",
    "# files.download(str(plot_file))\n",
    "\n",
    "print('\\nTo download files, uncomment the code above and run this cell.')\n",
    "print('\\nAlternatively, use the Files panel on the left to download manually.')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}